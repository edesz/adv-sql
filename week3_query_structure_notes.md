## Structuring of query to handle higher website traffic

1. Dropping duplicated events (event_timestamps) within each session is expensive and it will become a bigger problem when website traffic increases. Dropping duplicates is not necessary to correctly calculate the four metrics required in the report
   - (question 1) count the number of unique sessions
     - only the session_id column is needed to count the number of unique sessions. The session_id is the same for all events in the session, regardless of whether some event_timestamps are duplicated or not.
   - (question 2) get the average session length on a daily basis
     - The MIN() and MAX() functions pick up the event_timestamps of the first and last events respectively. The difference between these two timestamps is sufficient to calculate session length. The duplicates occur at the start of the session, which impacts the use of the MIN() function. However, since event_timestamp is the same for the duplicated events, the MIN() function correctly picks up the first event of the session even if duplicated events are present in the table.
   - (question 3) count the number of searches performed before viewing a recipe
     - only events corresponding to views of the home page (which occurred at the start of each session) were found to be duplicated and not events that were logged later in a session when a user performed a search or viewed a recipe. Checking if a recipe was viewed and counting the number of searches performed before viewing the recipe do not need to take home page views into account. Filtering the JSON of the event_details column is sufficient here, regardless of whether the events in each session before the user search are duplicated.
   - (question 4) get the ID of the most viewed recipe
     - user views of the recipe only occurs at the end of a session. So first event of the session is not needed to extract this ID. Duplicates only occurr for the first event of the session.
2. Redundant columns are present in the website_activity table and are excluded in all CTEs. Including such columns in CTEs will become more expensive as website traffic increases. This is especially important in the first CTE (in step 1. of the query) which returns output at the events level. Redundancy is handled as follows
   - the event_timestamp is used to calculate the average session length in seconds on a daily basis. If the event_timestamp column is used from the website_activity table then the event_id column is redundant. Also, the report does not require metrics at the event level. So, event_id is excluded.
   - the session_id column is used to count the number of daily unique sessions. Each user session is assigned a unique session_id. Also, the report does not require a count of the number of users. So, user_id is excluded.
3. Event processing is only performed in the first CTE (session_last_event_all_events_json). Parsing JSON for all events is expensive and will become slower as website traffic increases, so it is only performed in this first CTE in order to convert the event_details column into a VARIANT type. No additional datatype conversion operations are performed in the overall query. All other CTEs (except the first CTE) return aggregated outputs at the session of day level.
4. String functions are expensive and will be a bigger performance hit as the number of users on Virtual Kitchen increases. One string operation is needed: to clean the recipe_id by removing the leading and trailing ". Multiple recipes might be viewed every day. To minimize the use of string functions, the most-viewed recipe(s) are first identified and then the recipe_id is cleaned.
5. Only the best recipe_ids per day are returned by the most_viewed_recipe_daily CTE using QUALIFY. This avoids duplication of the window logic to get the most viewed recipe(s) daily between this CTE and the final CTE (daily_report_summary_sessions_recipe). This should help to improve performance when the number of users grows.
